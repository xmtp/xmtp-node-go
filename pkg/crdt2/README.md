This package implements topics as Merkle-Clocks based on [this paper](https://research.protocol.ai/publications/merkle-crdts-merkle-dags-meet-crdts/psaras2020.pdf). It attempts to treat topics as autonomous units although the underlying supporting components (store, broadcaster, syncer) will likely be shared among them.

To unpack in broad strokes what "topic as Merkle-Clock" means, and to introduce the terminology used throughout, let's look at this topic state that was generated by randomly sending 10 `messages` through a network of 3 `nodes`.

![t0](https://user-images.githubusercontent.com/871693/214625854-ef5c9166-1c19-433d-8888-90cbb8ab54da.jpg)

Each oval captures a single message, it is an `Event` of the clock. The arrows point at the most recent preceding events (the `heads`) known to the node that received that message from a client. In other words the arrows represent "happened after" relationships between the Events as observed by the individual nodes. Collectively the arrows coming out of an Event represent the `links` of the Event to its immediately preceding Events. This whole directed acyclic graph (DAG) forms a clock in the sense that it denotes a partial order of a sequence of Events captured by the "happened before" links. It is partial, because for example we don't know whether Event 4 happened before Event 5 or vice versa, but we do know that both happened after Event 2 and 3, and both happened before event 8.

What makes this clock a Merkle-Clock are the long (shortened in the picture) hex sequences associated with each Event. These are content identifiers, `CIDs`, which

1. uniquely identify each Event
2. capture the Event payload (the message is hashed into the CID)
3. capture the Event links (the CIDs of the linked events are also hashed into the CID)

The goal of the topic as Merkle-Clock is to support eventually consistent replication of topic contents among the nodes. Messages can be received, i.e. new Events to be created, by any node at any time. We want to make sure that all nodes participating in the topic eventually have all the events that happened across the network. Moreover we also want to make sure that all the nodes agree on the ordering of the events, i.e. given enough time they will all end up with the same DAG. The CIDs make it easy for nodes to see whether they already have given Event and whether they are missing any of its links. The CIDs also make things immutable, which allows the nodes to stop following the missing link chain once they reach known events.

Replication is mediated by three supporting facilities: the store, the broadcaster and the syncer. The `store` simply persists Events on a node. It allows the node to serve the Events it has to its clients and to other nodes. The `broadcaster` is used by the nodes to announce new Events they create to the rest of the network. However the broadcasts are not reliable, they may not reach some nodes due to transient network issues or due to nodes being turned off temporarily. This creates the situation where a node may receive a new event in a broadcast and realize that some of the links of the Event are unknown to it. This is where the `syncer` comes in, the nodes can use it to request missing events from the rest of the network. This allows the nodes to backfill their DAGs and catch up with the others.

The implementation here follows this broad structure, you can find the `Event`, `Topic`, `Node` as well as the `Store`, `Broadcaster` and `Syncer`.

## TODO

* simple timestamp event ordering
* querying
* adding/removing nodes to/from network
* restarting a node correctly (from store)
* investigate possible lockups
* boltdb store
* pubsub broadcaster
* libp2p syncer
* metrics
* fetching multiple cids at a time
* fetching multiple topics at a time
* validation
* consistent ordering respecting the DAG
* deep linking improve DAG sync speed
* rebroadcasting heads to catch up dormant topics
* what if anything should we do about inactive/dormant topics
* verify correctness: is there any scenario where nodes may fail to fully sync?